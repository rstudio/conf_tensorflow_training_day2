<!DOCTYPE html>
<html>
  <head>
    <title>Object detection</title>
    <meta charset="utf-8">
    <meta name="author" content="Sigrid Keydana" />
    <link rel="stylesheet" href="theme/rstudio.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Object detection
### Sigrid Keydana
### rstudio::conf 2019

---







# Learning objectives

- Perform classification and localization on a single object 

- Important concepts in multiple-object detection

- Understand how to code a _very_ basic version of SSD (__Single-Shot Multibox Detector__)

- Understand the options to improve on this basic detector

---
class: inverse, middle, center

# Road to object detection

---
# Single-object classification and localization

- Mix of recap (if you've participated in yesterday's workshop) and new topics

- Partly demo, partly exercise

- We'll also look at the dataset and preprocessing required for this session


---
# PASCAL Visual Object Classes (VOC) challenges and datasets

- Challenges (2005-2012) included
 - classification (presence/absence for each object class)
 - object detection (same as above, plus localization)
 - class segmentation
 - "person layout"
 
- We'll use the training set from the [2007 challenge](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html)

- Number of object classes: 20

- Number of training images: 2501

- We focus on concepts and the how-to, not accuracy

---
# Object detection examples

![](2_object_detection/images/birds_scaled.png)

![](2_object_detection/images/bicycles_scaled.png)


---
# How do you learn bounding boxes?

- Can be framed as __regression problem__

 - often trained with mean absolute error

- Predict pixel coordinates of box corners (_x_left_, _y_top_, _x_right_, _y_bottom_)

- Relevant metric is __Intersection over Union__ (__IOU__), also known as Jaccard index&lt;sup&gt;1&lt;/sup&gt;


.footnote[[1] Image source: Wikipedia.]

![](2_object_detection/images/iou.png)

---
# Demo/exercise: Single-object classification and localization

- Notebook: [2_object_detection/1_classification_localization.Rmd](2_object_detection/1_classification_localization.Rmd)

- Quiz: [2_object_detection/object_detection_quizzes.Rmd](2_object_detection/object_detection_quizzes.Rmd)


---
class: inverse, middle, center

# Introduction to multiple-object detection


---
# Why can't we just have more bounding boxes?


Each bounding box detector will try to detect all objects:&lt;sup&gt;1&lt;/sup&gt;

![](2_object_detection/images/2boxes.jpg)

We need to either:

- zoom in on single objects in some way, or
- have detectors __specialize__ in what to detect

.footnote[[1] Image source: http://machinethink.net/blog/object-detection/]

---
# Object detection main approaches / paradigms

- Sliding windows approaches
 - Train network, run sequentially on image patches 
 - May actually run sliding windows synchronously (see _Overfeat_ below)

- Region proposal approaches (2-step)
 - Step 1: Some algorithm proposes interesting regions
 - Step 2: Another algorithm (a convnet) classifies the regions and refines localization

- Single-shot detectors (YOLO, SSD)
 - Perform detection, classification and localization in one step

---
# Sliding windows done synchronously



.footnote[[1] cf. Sermanet, P, D. Eigen, X. Zhang, et al. (2013). OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. ]


![](2_object_detection/images/overfeat.png)

---
# Region proposal approaches

- R-CNN&lt;sup&gt;1&lt;/sup&gt;: Uses non-DL algorithm to select interesting regions, then applies CNN to all identified regions sequentially

- Fast R-CNN&lt;sup&gt;2&lt;/sup&gt;: Uses non-DL algorithm to select interesting regions, then classifies all regions in one pass

- Faster R-CNN&lt;sup&gt;3&lt;/sup&gt;: Uses a convnet for region proposal (_Region proposal network_), then classifies all regions in one pass



&lt;span class="footnote"&gt;
[1] cf. Girshick, R. B. (2015). Fast R-CNN.&lt;br /&gt;
[2] cf. Girshick, R. B, J. Donahue, T. Darrell, et al. (2013). Rich feature hierarchies for accurate object detection and semantic segmentation.&lt;br /&gt;
[3] cf. Ren, S, K. He, R. B. Girshick, et al. (2015). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.
&lt;/span&gt;




---
# Single-shot detectors

Make detectors __specialize__ using __anchor boxes__&lt;sup&gt;1&lt;/sup&gt;

![](2_object_detection/images/ssd_1.png)



.footnote[[1] illustration from: Liu, W, D. Anguelov, D. Erhan, et al. (2015). SSD: Single Shot MultiBox Detector.]


---
# Why do anchor boxes help?

### Concept 1: Grid

By laying a grid over the image, we _map detectors to image regions_.

### Concept 2: Different aspect rations and scales

Objects of different shapes are localized more easily because we lean on different customized _priors_.

&lt;br /&gt;

&lt;img src="2_object_detection/images/ssd_1.png" width = "50%"&gt;

---
# SSD architecture

&lt;br /&gt;

How do we detect objects that are of completely different sizes than a grid cell (e.g., span the whole image)?

SSD adds anchor boxes at different resolutions:

&lt;img src="2_object_detection/images/ssd_arch.png" width = "120%"&gt;

---
# SSD vs. YOLO

- Early YOLO versions (v1/v2) had dense layers before the final output, but current YOLO is fully convolutional just like SSD

- In addition to _number of classes_ class scores and 4 bounding box coordinates __per detector__, YOLO also has a _confidence score_

- YOLO determines anchor boxes for each dataset individually (using k-means clustering on the actual bounding boxes), while SSD uses _default boxes_

- Details of bounding box computations differ


---
# Why fully convolutional?

As long for every detector, the part of the image it's responsible for is visible (in its receptive field), we don't need a fully connected layer at the end&lt;sup&gt;1&lt;/sup&gt;

&lt;br /&gt;

&lt;img src="2_object_detection/images/receptive_field.png" width = "50%"/&gt;

.footnote[[1] Image from: Goodfellow, I, Y. Bengio and A. Courville (2016). Deep Learning.]

---
class: inverse, middle, center

# Coding a (very!) basic single-shot detector


---
# Basic SSD: Code

- To show the basic approach, we will
 - restrict ourselves to a 4x4 grid of image cells
 - have one anchor box per cell (thus, 16 anchor boxes)
 - don't aggregate detections from different resolutions 

- Notebook: [2_object_detection/2_object_detection_ssd.Rmd](2_object_detection/2_object_detection_ssd.Rmd)


---
# Basic SSD: Ways for improvement

- Use focal loss

- Use anchor boxes of different aspect rations

- Perform detection at various resolutions

- (not object detection specific:) Use data augmentation

---
# References

Girshick, R. B. (2015). "Fast R-CNN". In: _CoRR_ abs/1504.08083.
eprint: 1504.08083. URL:
[http://arxiv.org/abs/1504.08083](http://arxiv.org/abs/1504.08083).

Girshick, R. B, J. Donahue, T. Darrell, et al. (2013). "Rich
feature hierarchies for accurate object detection and semantic
segmentation". In: _CoRR_ abs/1311.2524. eprint: 1311.2524. URL:
[http://arxiv.org/abs/1311.2524](http://arxiv.org/abs/1311.2524).

Liu, W, D. Anguelov, D. Erhan, et al. (2015). "SSD: Single Shot
MultiBox Detector". In: _CoRR_ abs/1512.02325. eprint: 1512.02325.
URL:
[http://arxiv.org/abs/1512.02325](http://arxiv.org/abs/1512.02325).

Ren, S, K. He, R. B. Girshick, et al. (2015). "Faster R-CNN:
Towards Real-Time Object Detection with Region Proposal Networks".
In: _CoRR_ abs/1506.01497. eprint: 1506.01497. URL:
[http://arxiv.org/abs/1506.01497](http://arxiv.org/abs/1506.01497).

Sermanet, P, D. Eigen, X. Zhang, et al. (2013). "OverFeat:
Integrated Recognition, Localization and Detection using
Convolutional Networks". In: _CoRR_ abs/1312.6229. eprint:
1312.6229. URL:
[http://arxiv.org/abs/1312.6229](http://arxiv.org/abs/1312.6229).

---
# References (cont.)


```
## Warning in `[[.BibEntry`(x, ind): subscript out of bounds
```
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
