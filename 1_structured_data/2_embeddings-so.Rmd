---
title: "Entity embeddings 1: Extracting relationships"
output:
  html_notebook:
editor_options: 
  chunk_output_type: inline
---


### The data



```{r}
library(dplyr)
library(ggplot2)
library(readr)
library(keras)
library(purrr)
library(forcats)
library(ggrepel)

use_session_with_seed(7777, disable_gpu = FALSE, disable_parallel_cpu = FALSE)

unzip("data/survey_results_public.csv.zip", exdir = "data/")

data <- read_csv("data/survey_results_public.csv", progress = FALSE)
```

We zoom in on a few categorical variables. For our demo, we want to try to predict ethical convictions/behavior (as measured by one of four ethics-related questions), as well as job satisfaction.
Thus, in the pick of variables below we plan to use `EthicsChoice` and `JobSatisfaction` as targets, the others as predictors.

```{r}
data <- data %>% select(
  FormalEducation,
  UndergradMajor,
  starts_with("AssessJob"),
  EthicsChoice,
  LanguageWorkedWith,
  OperatingSystem,
  EthicsChoice,
  JobSatisfaction
)

data <- data %>% mutate_if(is.character, factor)
```

### Variable contents/formats

```{r}
data %>% glimpse()
```

Let's switch to `survey_results_schema.csv` to see what these variables are.

For the predictors, we have 3 different formats here:

- "normal" (single-choice) categorical variables (`FormalEducation`, `UndergradMajor`, `OperatingSystem`)
- multiple-choice categorical variables (`LanguageWorkedWith`)
- rankings (`AssessJob1` etc.)

We will need to think about how we can meaningfully encode them.


### Preprocessing / variable encoding

The variables we are interested in show a tendency to have been left unanswered by quite a few respondents, so the easiest way to handle missing data here is to exclude the respective participants completely.

```{r}
data <- na.omit(data)
```

That leaves us with ~48,000 completed (as far as we're concerned) questionnaires.
Looking at the variables' contents, we see we'll have to do something with them before we can start training.

```{r}
data %>% glimpse()
```

#### Target variables

We want to binarize both target variables. Let's inspect them, starting with `EthicsChoice`.

```{r}
jslevels <- levels(data$JobSatisfaction)
elevels <- levels(data$EthicsChoice)

data <- data %>% mutate(
  JobSatisfaction = JobSatisfaction %>% fct_relevel(
    jslevels[1],
    jslevels[3],
    jslevels[6],
    jslevels[5],
    jslevels[7],
    jslevels[4],
    jslevels[2]
  ),
  EthicsChoice = EthicsChoice %>% fct_relevel(
    elevels[2],
    elevels[1],
    elevels[3]
  ) 
)

ggplot(data, aes(EthicsChoice)) + geom_bar()
```


You might agree that with a question containing the phrase _a purpose or product that you consider extremely unethical_, the answer "depends on what it is" feels closer to "yes" than to "no". If that seems like too skeptical a thought, it's still the only binarization that achieves a sensible split.

```{r}
data <- data %>% mutate(
  EthicsChoice = if_else(as.numeric(EthicsChoice) == 2, 1, 0)
  )
```



Looking at our second target variable, `JobSatisfaction`:

```{r}
ggplot(data, aes(JobSatisfaction)) + geom_bar()
```



We think that given the mode at "moderately satisfied", a sensible way to binarize is a split into "moderately satisfied" and "extremely satisfied" on one side, all remaining options on the other:

```{r}
data <- data %>% mutate(
  JobSatisfaction = if_else(as.numeric(JobSatisfaction) > 5, 1, 0)
  )
```


#### Predictors

##### Single-choice (`FormalEducation`, `UndergradMajor` and `OperatingSystem`)

These can simply be one-hot-encoded.
For curiosity's sake, let's look at how they're distributed:

```{r}
data %>% group_by(FormalEducation) %>%
  summarise(count = n()) %>%
  arrange(desc(count))
```


```{r}
data %>% group_by(UndergradMajor) %>%
  summarise(count = n()) %>%
  arrange(desc(count))
```


```{r}
data %>% group_by(OperatingSystem) %>%
  summarise(count = n()) %>%
  arrange(desc(count))
```


##### `LanguageWorkedWith` (multiple-choice)

`LanguageWorkedWith`contains sequences of programming languages, concatenated by semicolon.
There are several ways to deal with this.
One way to unpack these is using Keras' `text_tokenizer`.

```{r}
language_tokenizer <- text_tokenizer(split = ";", filters = "")
language_tokenizer %>% fit_text_tokenizer(data$LanguageWorkedWith)
```


```{r}
language_tokenizer$word_counts
```


We have 38 languages overall. We're curious to see what people work with, of course:

```{r}
data.frame(
  name = language_tokenizer$word_counts %>% names(),
  count = language_tokenizer$word_counts %>% unlist() %>% unname()
) %>%
 arrange(desc(count))
```


Now `texts_to_matrix` will nicely create a multi-hot representation of the multiple-choice column.

```{r}
langs <- language_tokenizer %>%
  texts_to_matrix(data$LanguageWorkedWith, mode = "count")
langs[1:3, ]
```



We can simply append these columns to the dataframe (and do a little cleanup):

```{r}
data <- data %>% cbind(langs[, 2:39]) # the very first column is not useful
data <- data %>% rename_at(vars(`1`:`38`), funs(paste0(language_tokenizer$index_word[as.integer(.)])))
data <- data %>% select(-LanguageWorkedWith)
```


##### AssessJob[n] (multiple rankings)

For quick reference, hese were the features that had to be ranked:

> The industry that I'd be working in

> The financial performance or funding status of the company or organization

> The specific department or team I'd be working on

> The languages, frameworks, and other technologies I'd be working with

> The compensation and benefits offered

> The office environment or company culture

> The opportunity to work from home/remotely

> Opportunities for professional development

> The diversity of the company or organization

> How widely used or impactful the product or service I'd be working on is


Columns `AssessJob1` to `AssessJob10` contain the respective ranks, that is, values between 1 and 10.

Here, we need to find a way to preprocess this column that "makes sense" on the one hand, and we know how to technically do it on the other hand. The following procedure is just a suggestion - you can do it differently in the exercise if you want.

Based on introspection about the cognitive effort to actually establish an order among 10 items, we decided to pull out the three top-ranked features per person and treat them as equal. 

As a first step, we extract and concatenate these:


```{r}
data <- data %>% mutate(
  val_1 = if_else(
   AssessJob1 == 1, "industry", if_else(
    AssessJob2 == 1, "company_financial_status", if_else(
      AssessJob3 == 1, "department", if_else(
        AssessJob4 == 1, "languages_frameworks", if_else(
          AssessJob5 == 1, "compensation", if_else(
            AssessJob6 == 1, "company_culture", if_else(
              AssessJob7 == 1, "remote", if_else(
                AssessJob8 == 1, "development", if_else(
                  AssessJob10 == 1, "diversity", "impact"))))))))),
  val_2 = if_else(
    AssessJob1 == 2, "industry", if_else(
      AssessJob2 == 2, "company_financial_status", if_else(
        AssessJob3 == 2, "department", if_else(
          AssessJob4 == 2, "languages_frameworks", if_else(
            AssessJob5 == 2, "compensation", if_else(
              AssessJob6 == 2, "company_culture", if_else(
                AssessJob7 == 1, "remote", if_else(
                  AssessJob8 == 1, "development", if_else(
                    AssessJob10 == 1, "diversity", "impact"))))))))),
  val_3 = if_else(
    AssessJob1 == 3, "industry", if_else(
      AssessJob2 == 3, "company_financial_status", if_else(
        AssessJob3 == 3, "department", if_else(
          AssessJob4 == 3, "languages_frameworks", if_else(
            AssessJob5 == 3, "compensation", if_else(
              AssessJob6 == 3, "company_culture", if_else(
                AssessJob7 == 3, "remote", if_else(
                  AssessJob8 == 3, "development", if_else(
                    AssessJob10 == 3, "diversity", "impact")))))))))
  )

data <- data %>% mutate(
  job_vals = paste(val_1, val_2, val_3, sep = ";") %>% factor()
)

data <- data %>% select(
  -c(starts_with("AssessJob"), starts_with("val_"))
)

data$job_vals[1:10]
```


Now, that column looks exactly like `LanguageWorkedWith` looked before, so we can use the same method as above to produce a one-hot-encoded version.

```{r}
values_tokenizer <- text_tokenizer(split = ";", filters = "")
values_tokenizer %>% fit_text_tokenizer(data$job_vals)
```

Now, using the same method as above:

```{r}
job_values <- values_tokenizer %>% texts_to_matrix(data$job_vals, mode = "count")
data <- data %>% cbind(job_values[, 2:11])
data <- data %>% rename_at(vars(`1`:`10`), funs(paste0(values_tokenizer$index_word[as.integer(.)])))
data <- data %>% select(-job_vals)
```


we end up with a dataset that looks like this:

```{r}
data %>% glimpse()
```


which we further reduce to a design matrix `X` removing the binarized target variables 

```{r}
X <- data %>% select(-c(JobSatisfaction, EthicsChoice))
```

Now we are ready to continue either with or without embeddings. Our focus is on embeddings here, but we list the one-hot code just for reference.

Just in case you wanted to compare performance with and without embeddings, we do the train-test split in advance for both:

```{r}
train_indices <- sample(1:nrow(X), 0.8 * nrow(X))
```


### One-hot model

For the one-hot model, all that remains to be done is using Keras' `to_categorical` on the three remaining variables that are not yet in one-hot form.

```{r}
X_one_hot <- X %>% map_if(is.factor, ~ as.integer(.x) - 1) %>%
  map_at("FormalEducation", ~ to_categorical(.x) %>% 
           array_reshape(c(length(.x), length(levels(data$FormalEducation))))) %>%
  map_at("UndergradMajor", ~ to_categorical(.x) %>% 
           array_reshape(c(length(.x), length(levels(data$UndergradMajor))))) %>%
  map_at("OperatingSystem", ~ to_categorical(.x) %>%
           array_reshape(c(length(.x), length(levels(data$OperatingSystem))))) %>%
  abind::abind(along = 2)
```

We divide up our dataset into train and validation parts

```{r}
x_train <- X_one_hot[train_indices, ] %>% as.matrix()
x_valid <- X_one_hot[-train_indices, ] %>% as.matrix()
y_train <- data$EthicsChoice[train_indices] %>% as.matrix()
y_valid <- data$EthicsChoice[-train_indices] %>% as.matrix()
```

and define a pretty straightforward MLP.

```{r}
model <- keras_model_sequential() %>%
  layer_dense(
    units = 128,
    activation = "relu"
  ) %>%
  layer_dropout(0.5) %>%
  layer_dense(
    units = 128,
    activation = "relu"
  ) %>%
  layer_dropout(0.5) %>%
  layer_dense(
    units = 128,
    activation = "relu"
  ) %>%
  layer_dropout(0.5) %>%
  layer_dense(
    units = 128,
    activation = "relu"
  ) %>%
  layer_dropout(0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
  )
```


Training this model:

```{r}
history <- model %>% fit(
  x_train,
  y_train,
  validation_data = list(x_valid, y_valid),
  epochs = 20,
  batch_size = 100
)

plot(history)
```

...results in an accuracy on the validation set of ~ 0.65 - not an impressive number per se, but interesting given the small amount of predictors and the choice of target variable. 


### Embeddings model

In the embeddings model, we don't need to use `to_categorical` on the remaining factors, as embedding layers can work with integer input data. We thus just convert the factors to integers:

```{r}
X_embed <- X %>%
  mutate_if(is.factor, compose(partial(`-`, 1, .first = FALSE), as.integer))
```

Now for the model. Effectively we have five groups of entities here: formal education, undergrad major, operating system, languages worked with, and highest-counting values with respect to jobs. Each of these groups get embedded separately, so we need to use the Keras functional API and declare five different inputs.

```{r}
input_fe <- layer_input(shape = 1)        # formal education, encoded as integer
input_um <- layer_input(shape = 1)        # undergrad major, encoded as integer
input_os <- layer_input(shape = 1)        # operating system, encoded as integer
input_langs <- layer_input(shape = 38)    # languages worked with, multi-hot-encoded
input_vals <- layer_input(shape = 10)     # values, multi-hot-encoded
```

Having embedded them separately, we concatenate the outputs for further common processing.

```{r}
concat <- layer_concatenate(
  list(
    input_fe %>%
      layer_embedding(
        input_dim = length(levels(data$FormalEducation)),
        output_dim = 64,
        name = "fe"
      ) %>%
      layer_flatten(),
    input_um %>%
      layer_embedding(
        input_dim = length(levels(data$UndergradMajor)),
        output_dim = 64,
        name = "um"
      ) %>%
      layer_flatten(),
    input_os %>%
      layer_embedding(
        input_dim = length(levels(data$OperatingSystem)),
        output_dim = 64,
        name = "os"
      ) %>%
      layer_flatten(),
    input_langs %>%
       layer_embedding(input_dim = 38, output_dim = 256,
                       name = "langs")%>%
       layer_flatten(),
    input_vals %>%
      layer_embedding(input_dim = 10, output_dim = 128,
                      name = "vals")%>%
      layer_flatten()
  )
)

output <- concat %>%
  layer_dense(
    units = 128,
    activation = "relu"
  ) %>%
  layer_dropout(0.5) %>%
  layer_dense(
    units = 128,
    activation = "relu"
  ) %>%
  layer_dropout(0.5) %>%
  layer_dense(
    units = 128,
    activation = "relu"
  ) %>%
  layer_dense(
    units = 128,
    activation = "relu"
  ) %>%
  layer_dropout(0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")

```


So there go model definition and compilation:

```{r}
model <- keras_model(list(input_fe, input_um, input_os, input_langs, input_vals), output)

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
  )
```


Now to pass the data to the model, we need to chop it up into ranges of columns matching the inputs.

```{r}
y_train <- data$EthicsChoice[train_indices] %>% as.matrix()
y_valid <- data$EthicsChoice[-train_indices] %>% as.matrix()

x_train <-
  list(
    X_embed[train_indices, 1, drop = FALSE] %>% as.matrix() ,
    X_embed[train_indices , 2, drop = FALSE] %>% as.matrix(),
    X_embed[train_indices , 3, drop = FALSE] %>% as.matrix(),
    X_embed[train_indices , 4:41, drop = FALSE] %>% as.matrix(),
    X_embed[train_indices , 42:51, drop = FALSE] %>% as.matrix()
  )
x_valid <- list(
  X_embed[-train_indices, 1, drop = FALSE] %>% as.matrix() ,
  X_embed[-train_indices , 2, drop = FALSE] %>% as.matrix(),
  X_embed[-train_indices , 3, drop = FALSE] %>% as.matrix(),
  X_embed[-train_indices , 4:41, drop = FALSE] %>% as.matrix(),
  X_embed[-train_indices , 42:51, drop = FALSE] %>% as.matrix()
)
```


And we're ready to train.

```{r}
model %>% fit(
  x_train,
  y_train,
  validation_data = list(x_valid, y_valid),
  epochs = 20,
  batch_size = 100
)
```


Using the same train-test split as before, this results in an accuracy of ... ~0.65 (just as before). 
But in this use case, we're mainly interested in extracting relationships from the learned embeddings.


#### Extracting relationships from the learned embeddings

We'll show the code here for the _job values_ embeddings, - it is directly transferable to the other ones.
_The embeddings_, that's just the weight matrix of the respective layer, of dimension `number of different values` times `embedding size`.

```{r}
emb_vals <- (model$get_layer("vals") %>% get_weights())[[1]]
emb_vals %>% dim() 
```

We can then perform dimensionality reduction on the raw values, e.g., PCA

```{r}
pca <- prcomp(emb_vals, center = TRUE, scale. = TRUE, rank = 2)$x[, c("PC1", "PC2")]
```


and plot the results.

```{r}
pca %>%
  as.data.frame() %>%
  mutate(class = attr(values_tokenizer$word_index, "names")) %>%
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point() +
  geom_label_repel(aes(label = class))
```


When looking at the results, please keep in mind:

- Accuracy on the prediction task that lead to these embedding matrices was pretty modest - this means we shouldn't take the embeddings results too seriously, either.
- To obtain these embeddings, we've made use of an extraneous variable (the ethical behavior to be predicted). So any learned relationships are never "absolute", but always to be seen in relation to the way they were learned. This is why we chose an additional target variable, `JobSatisfaction`, for you to compare with.
