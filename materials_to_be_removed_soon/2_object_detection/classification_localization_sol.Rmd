---
title: "Exercise: Classification and localization"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

```{r}
library(keras)
library(rjson)
library(magick)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
```


## Dataset

We'll be using images and annotations from the _Pascal VOC dataset_.
The images are here


```{r}
img_dir <- "data/VOCdevkit/VOC2007/JPEGImages"
```

and the annotations are here

```{r}
annot_file <- "data/pascal_train2007.json"
```


We need to extract image-related data from that _json_ file.

We're presenting the steps here for you to execute and keep track of the data.
Later, it'll be your turn to code the neural network stuff.

## Preprocessing

Annotations contain information about three types of things we're interested in.

```{r}
annotations <- fromJSON(file = annot_file)
str(annotations, max.level = 1)
```


First, we need the characteristics of the image itself (height and width) and where it's stored. Not surprisingly, here it's one entry per image.

```{r}
imageinfo <- annotations$images %>% {
  tibble(
    id = map_dbl(., "id"),
    file_name = map_chr(., "file_name"),
    image_height = map_dbl(., "height"),
    image_width = map_dbl(., "width")
  )
}
```


Then, object class ids and bounding box coordinates. There may be multiple of these per image.
In Pascal VOC, there are 20 object classes.

```{r}
classes <- c(
  "aeroplane",
  "bicycle",
  "bird",
  "boat",
  "bottle",
  "bus",
  "car",
  "cat",
  "chair",
  "cow",
  "diningtable",
  "dog",
  "horse",
  "motorbike",
  "person",
  "pottedplant",
  "sheep",
  "sofa",
  "train",
  "tvmonitor"
)

boxinfo <- annotations$annotations %>% {
  tibble(
    image_id = map_dbl(., "image_id"),
    category_id = map_dbl(., "category_id"),
    bbox = map(., "bbox")
  )
}
```

The bounding boxes are now stored in a list column and need to be unpacked.

```{r}
boxinfo <- boxinfo %>% 
  mutate(bbox = unlist(map(.$bbox, function(x) paste(x, collapse = " "))))
boxinfo <- boxinfo %>% 
  separate(bbox, into = c("x_left", "y_top", "bbox_width", "bbox_height"))
boxinfo <- boxinfo %>% mutate_all(as.numeric)
```

For the bounding boxes, the annotation file provides `x_left` and `y_top` coordinates, as well as width and height.
We will mostly be working with corner coordinates, so we create the missing `x_right` and `y_top`.

As usual in image processing, the `y` axis starts from the top.

```{r}
boxinfo <- boxinfo %>% 
  mutate(y_bottom = y_top + bbox_height - 1, x_right = x_left + bbox_width - 1)
```


Finally, we still need to match class ids to class names.

```{r}
catinfo <- annotations$categories %>%  {
  tibble(id = map_dbl(., "id"), name = map_chr(., "name"))
}
```

So, putting it all together:

```{r}
imageinfo <- imageinfo %>%
  inner_join(boxinfo, by = c("id" = "image_id")) %>%
  inner_join(catinfo, by = c("category_id" = "id"))
```

Note that here still, we have several entries per image, each annotated object occupying its own row.

Soon, we'll run these images through a model that takes a resolution of `224x224`. The bounding box coordinates need to be re-scaled to that size.

```{r}
target_height <- 224
target_width <- 224

imageinfo <- imageinfo %>% mutate(
  x_left_scaled = (x_left / image_width * target_width) %>% round(),
  x_right_scaled = (x_right / image_width * target_width) %>% round(),
  y_top_scaled = (y_top / image_height * target_height) %>% round(),
  y_bottom_scaled = (y_bottom / image_height * target_height) %>% round(),
  bbox_width_scaled =  (bbox_width / image_width * target_width) %>% round(),
  bbox_height_scaled = (bbox_height / image_height * target_height) %>% round()
)
```

We're ready. Let's take a glance at our data. Picking one of the early entries and displaying the original image together with the object annotation yields

```{r}
img_data <- imageinfo[4,]
img <- image_read(file.path(img_dir, img_data$file_name))
img <- image_draw(img)
rect(
  img_data$x_left,
  img_data$y_bottom,
  img_data$x_right,
  img_data$y_top,
  border = "white",
  lwd = 2
)
text(
  img_data$x_left,
  img_data$y_top,
  img_data$name,
  offset = 1,
  pos = 2,
  cex = 1.5,
  col = "white"
)
dev.off()
```

![](images/bicycle.jpeg){width=80%}

## Zooming in on one object


In this exercise, we'll do classification and localization of a single object, but as this is a multi-object dataset, we need to zoom in on one. A reasonable strategy seems to be choosing the object with the largest ground truth bounding box.
You might suspect now already that performance could turn out to be worse than had we chosen a single-object dataset.

```{r}
imageinfo <- imageinfo %>% mutate(area = bbox_width_scaled * bbox_height_scaled)

imageinfo_maxbb <- imageinfo %>%
  group_by(id) %>%
  filter(which.max(area) == row_number())
```


## Train-test split

Time for action! Split the dataset (`imageinfo_maxbb`) into 80% training and 20% validation.

```{r}
train_indices <- sample(1:n_samples, 0.8 * n_samples)
train_data <- imageinfo_maxbb[train_indices,]
validation_data <- imageinfo_maxbb[-train_indices,]
```

We start with classification and then look at localization.

## Single-object classification

### Data streaming

For classification, to pass our data to `keras`, simply use `image_data_generator`, but as soon as we'll start with bounding boxes we will need to build custom generators. So here, we provide a simple generator that delivers images as well as the corresponding targets in a stream. Later you can adapt this to the new tasks

Note how the targets are not one-hot-encoded, but integers - `keras` can handle provided we use the appropriate loss function here. You're free to change this - just make sure that whichever way you deliver the data to the model, the loss function matches.


```{r}
batch_size <- 10

# this preprocessing function will be called from the generator
load_and_preprocess_image <- function(image_name, target_height, target_width) {
  img_array <- image_load(
    file.path(img_dir, image_name),
    target_size = c(target_height, target_width)
    ) %>%
    image_to_array() %>%
    xception_preprocess_input() 
  dim(img_array) <- c(1, dim(img_array))
  img_array
}

# generator function
# can deliver both shuffled as well as non-shuffled data
# returns images and targets (classes) in a list
classification_generator <-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i <- 1
    function() {
      if (shuffle) {
        indices <- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size >= nrow(data))
          i <<- 1
        indices <- c(i:min(i + batch_size - 1, nrow(data)))
        i <<- i + length(indices)
      }
      x <-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y <- array(0, dim = c(length(indices), 1))
      
      for (j in 1:length(indices)) {
        x[j, , , ] <-
          load_and_preprocess_image(data[[indices[j], "file_name"]],
                                    target_height, target_width)
        y[j, ] <-
          data[[indices[j], "category_id"]] - 1
      }
      x <- x / 255
      list(x, y)
    }
  }

# training generator
train_gen <- classification_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

# validation generator
valid_gen <- classification_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)
```


### Single-object classification model

Now, code a model for image classification. We recommend using XCeption as a basic feature extractor. 
Make sure its weights are frozen to speed up the training process.

```{r}
feature_extractor <-
  application_xception(
    include_top = FALSE,
    input_shape = c(224, 224, 3),
    pooling = "avg"
)

feature_extractor %>% freeze_weights()
```

Now, put a few custom layers on top. At the end, you want to have a layer that allows you to decide between 20 classes.

```{r}
model <- keras_model_sequential() %>%
  feature_extractor %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 20, activation = "softmax")

```

Compile the model. Choose a loss function appropriate for the task and display metrics that help you see how training is going.

```{r}
model %>% compile(
  optimizer = "adam",
  loss = "sparse_categorical_crossentropy",
  metrics = list("accuracy")
)
```


### Training and predictions

Now train the model for a few epochs to make sure that loss decreases. We don't really have the time to train for a longer time in the workshop. 

```{r}
model %>% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_model_checkpoint(
      file.path("class_only", "weights.{epoch:02d}-{val_loss:.2f}.hdf5")
    ),
    callback_early_stopping(patience = 2)
  )
)
```

Try some predictions on the validation set.

```{r}
 model %>% predict(
      load_and_preprocess_image(train_data$filename[1], 
                                target_height, target_width)
 )
```



### Aside: Multiple object classification

We can't do everything in a few hours so let's just think about what would have to change if we wanted to detect multiple objects in an image.

- multi-hot-encoding
- use `sigmoid` loss
- use `binary_crossentropy`

On to working with a single object and a new task: localization.

## Single-object localization

### Model

Let's start with the model this time, so it's clear what the generator has to deliver. 

First, code the feature extractor, and freeze its weights. Use Xception, as before.
However, there's an important difference here: We want to keep all spatial information from the before-last layer, so don't do any pooling or flattening.


```{r}
feature_extractor <- application_xception(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)

feature_extractor %>% freeze_weights()
```


Now append a custom head. At the output layer, you want to have 4 units, one for each bounding box coordinate.

```{r}
model <- keras_model_sequential() %>%
  feature_extractor %>%
  layer_flatten() %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 4)
```


### Metrics

For the cost function, choose a loss common in regression. In addition, we'd like to keep track of (= use as a custom metric) a more tangible quantity: How much do estimate and ground truth overlap?

Overlap is usually measured as _Intersection over Union_, or _Jaccard distance_. Intersection over Union is exactly what it says, a ratio between space shared by the objects and space occupied when we take them together.

Here is an implementation. Use this as an additional metric when compiling the model (hint: wrap it in `custom_metric`):

```{r}
metric_iou <- function(y_true, y_pred) {
  
  # order is [x_left, y_top, x_right, y_bottom]
  intersection_xmin <- k_maximum(y_true[ ,1], y_pred[ ,1])
  intersection_ymin <- k_maximum(y_true[ ,2], y_pred[ ,2])
  intersection_xmax <- k_minimum(y_true[ ,3], y_pred[ ,3])
  intersection_ymax <- k_minimum(y_true[ ,4], y_pred[ ,4])
  
  area_intersection <- (intersection_xmax - intersection_xmin) * 
                       (intersection_ymax - intersection_ymin)
  area_y <- (y_true[ ,3] - y_true[ ,1]) * (y_true[ ,4] - y_true[ ,2])
  area_yhat <- (y_pred[ ,3] - y_pred[ ,1]) * (y_pred[ ,4] - y_pred[ ,2])
  area_union <- area_y + area_yhat - area_intersection
  
  iou <- area_intersection/area_union
  k_mean(iou)
  
}
```

Now compile the model.

```{r}
model %>% compile(
  optimizer = "adam",
  loss = "mae",
  metrics = list(custom_metric("iou", metric_iou))
)
```

### Data generator

Now modify the generator to return bounding box coordinates as targets...

```{r}
localization_generator <-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i <- 1
    function() {
      if (shuffle) {
        indices <- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size >= nrow(data))
          i <<- 1
        indices <- c(i:min(i + batch_size - 1, nrow(data)))
        i <<- i + length(indices)
      }
      x <-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y <- array(0, dim = c(length(indices), 4))
      
      for (j in 1:length(indices)) {
        x[j, , , ] <-
          load_and_preprocess_image(data[[indices[j], "file_name"]], 
                                    target_height, target_width)
        y[j, ] <-
          data[indices[j], c("x_left_scaled",
                             "y_top_scaled",
                             "x_right_scaled",
                             "y_bottom_scaled")] %>% as.matrix()
      }
      x <- x / 255
      list(x, y)
    }
  }

train_gen <- localization_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

valid_gen <- localization_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)
```

### Train the model

Train for a little time, just to see that the code works.

```{r}
model %>% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_model_checkpoint(
      file.path("loc_only", "weights.{epoch:02d}-{val_loss:.2f}.hdf5")
    ),
    callback_early_stopping(patience = 2)
  )
)
```

### Verify some predictions

Here's a convenience function that displays an image, the ground truth box of the most salient object (as defined above), and if given, class and bounding box predictions.

```{r}
plot_image_with_boxes <- function(file_name,
                                  object_class,
                                  box,
                                  scaled = FALSE,
                                  class_pred = NULL,
                                  box_pred = NULL) {
  img <- image_read(file.path(img_dir, file_name))
  if(scaled) img <- image_resize(img, geometry = "224x224!")
  img <- image_draw(img)
  x_left <- box[1]
  y_bottom <- box[2]
  x_right <- box[3]
  y_top <- box[4]
  rect(
    x_left,
    y_bottom,
    x_right,
    y_top,
    border = "cyan",
    lwd = 2.5
  )
  text(
    x_left,
    y_top,
    object_class,
    offset = 1,
    pos = 2,
    cex = 1.5,
    col = "cyan"
  )
  if (!is.null(box_pred))
    rect(box_pred[1],
         box_pred[2],
         box_pred[3],
         box_pred[4],
         border = "yellow",
         lwd = 2.5)
  if (!is.null(class_pred))
    text(
      box_pred[1],
      box_pred[2],
      class_pred,
      offset = 0,
      pos = 4,
      cex = 1.5,
      col = "yellow")
  dev.off()
  img %>% image_write(paste0("preds_", file_name))
  plot(img)
}
```

Now look at some predictions from the training set. Given you've probably not been training the model for so long, don't expect too much. In any case: Do you see a systematic problem here?


```{r}
train_1_8 <- train_data[1:8, c("file_name",
                               "name",
                               "x_left_scaled",
                               "y_top_scaled",
                               "x_right_scaled",
                               "y_bottom_scaled")]

for (i in 1:8) {
  preds <-
    model %>% predict(
      load_and_preprocess_image(train_1_8[i, "file_name"], 
                                target_height, target_width),
      batch_size = 1
  )
  plot_image_with_boxes(train_1_8$file_name[i],
                        train_1_8$name[i],
                        train_1_8[i, 3:6] %>% as.matrix(),
                        scaled = TRUE,
                        box_pred = preds)
}
```


![Sample bounding box predictions on the training set.](images/preds_train.jpg){width=100%}

We'll skip over predictions on the validation set and pass on to the last ask: doing both regression and classification in the same model.

## Single-object detection (classification + localization)

For time reasons, we won't do the actual training, but we look at the model and the data generator.

### Model for single-object detection

Combining regression and classification into one means we'll want to have two outputs in our model.
We'll thus use the functional API this time. 

Again, start with XCeption as a feature extractor.

```{r}
feature_extractor <- application_xception(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)
```

Then, we'll want a common custom head but at the end, two outputs, one for the class prediction and one for the bounding box regression.

```{r}


input <- feature_extractor$input
common <- feature_extractor$output %>%
  layer_flatten(name = "flatten") %>%
  layer_activation_relu() %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.5)

regression_output <-
  layer_dense(common, units = 4, name = "regression_output")
class_output <- layer_dense(
  common,
  units = 20,
  activation = "softmax",
  name = "class_output"
)

model <- keras_model(
  inputs = input,
  outputs = list(regression_output, class_output)
)
```

Then in `compile`, the losses will have to be a list, like so.
You'd also want to experiment with weighting losses differently.

```{r}
model %>% freeze_weights(to = "flatten")

model %>% compile(
  optimizer = "adam",
  loss = list("mae", "sparse_categorical_crossentropy"),
  #loss_weights = list(
  #  regression_output = 0.05,
  #  class_output = 0.95),
  metrics = list(
    regression_output = custom_metric("iou", metric_iou),
    class_output = "accuracy"
  )
)
```


We'll stop here and move on to (multiple) object detection.